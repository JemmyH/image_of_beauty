## 一、 爬取的网站：
网站地址：http://www.xgyw.cc （这个网站好像被注入了JavaScript脚本，如果在Firefox或者chrome直接打开会跳转到另一个不可描述的网站，所以，请使用Microsoft Edge浏览器并使用必应搜索引擎打开————没想到微软反人类的设计居然成就了这么一件好事……）
## 二、 实现的功能：
根据主页对图片的分类逐项爬取，最后获得每一张图片的url，再调用七牛云的Python SDK将图片url储存在七牛云上。
## 三、 代码编写过程中的收获：
1. 这个网站采取的反爬虫措施很复杂，如果访问频率过快或者直接使用urllib.urlretrieve方法，会直接遇到403 forbidden、或“主机拒绝了访问”，因此总觉得走直路可能行不通，得另想办法；
2. 针对直接使用urllib.urlretrieve方法不能下载，我在知乎上找到一个方法（抱歉原文链接没找到），我写在了download.new_download方法里面，可以下载，但是如果访问频率过快或者访问次数过多也会被主机拒绝访问；
3. 我原本打算将图片url全部储存在本地的mysql数据库中，但是一想意义不大，放弃这条路；
4. 最后在知乎上发现了“外链”这个说法，根据他们推荐我主要针对了两个云产品：百度云和七牛云。查了一下七牛云的SDK（https://developer.qiniu.com/kodo/sdk/1242/python），果断入手。
5. urllib.request.urlopen()一定要设置timeout并通过try except捕捉，不然爬虫运行过程中很容易出现“假死”（到某一个链接不动了）
## 四、 其他的解决方案：
1. 使用代理服务器和添加浏览器user-agent。这是最基本的反爬虫策略，可是我遇到的问题是：我能通过urllib.request.urlopen()打开网页并获取数据，但是就是不能使用urllib.request.urlretrieve()下载图片，使用代理服务器最终还是绕不过这一步，所以我就没有尝试代理服务器；如上所说，修改head后可以下载图片，但是仅有少数几张，意义不大，也放弃；
2. 使用Python selenium包（https://www.seleniumhq.org/）  ，这个包的作用是：通过代码启动浏览器，并能够模仿浏览器进行一定的操作。这就提供了一条思路：用浏览器直接图片url打开图片，然后右键、“另存为”就可以保存到本地。可是实践过程中又遇到了让人掉头发的事：出现“另存为”窗口后，这个窗口应该是属于Windows而不是浏览器了，所以selenium无法做出点击“保存”的动作，总不能人为去点击吧……这个时候伟大的csdn上又有人推荐了这个神器——aotoit软件，它的功能是：将Windows窗口的点击事件获取，并模仿人的点击操作。说到这里你应该明白我说的意思，按道理说这个思路是可行的。
3. 使用百度云网页端的“离线下载功能”。这里也有百度云的SDK（https://github.com/solos/baidupan） ,我手动试了一下离线下载，可行。
## 五、 其他
等想到了再补充
